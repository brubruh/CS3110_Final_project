{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "268f1876",
   "metadata": {},
   "source": [
    "## Goals and Privacy Requirements\n",
    "- Task: create synthetic user locations that preserve aggregate patterns while ensuring differential privacy.\n",
    "- Mechanism: Laplace noise on gridded counts; privacy budget split between counts and release.\n",
    "- Risk controls: k-anonymity / l-diversity style checks on city buckets; t-closeness-inspired distance between raw and synthetic distributions.\n",
    "- Utility checks: distribution similarity (Jensen-Shannon), spatial error summaries, visual overlays.\n",
    "- Reproducible single-file pipeline: run top-to-bottom to produce data, metrics, and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (top of notebook per README guidance)\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da21f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for reproducibility and privacy budgets\n",
    "@dataclass\n",
    "class DPConfig:\n",
    "    seed: int = 7\n",
    "    grid_size: int = 50              # square grid resolution over bounding box\n",
    "    epsilon_counts: float = 1.0      # privacy budget for gridded counts\n",
    "    epsilon_release: float = 0.5     # budget for releasing synthetic points per cell\n",
    "    jitter_std_km: float = 1.0        # noise around cell centers when sampling synthetic points\n",
    "    k_anonymity_k: int = 10\n",
    "    l_diversity_l: int = 2\n",
    "    t_closeness_thresh: float = 0.2  # max JS divergence between raw and synthetic per bucket\n",
    "\n",
    "config = DPConfig()\n",
    "rng = np.random.default_rng(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e54ff",
   "metadata": {},
   "source": [
    "## Step 1 — Build a realistic raw dataset (synthetic but non-private)\n",
    "We emulate raw user check-ins across several metro areas (no real PII). Each user is assigned a home city center and jittered around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metro centers (lat, lon) and relative popularity\n",
    "metros = [\n",
    "    {\"city\": \"New York\", \"lat\": 40.7128, \"lon\": -74.0060, \"weight\": 1.0},\n",
    "    {\"city\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437, \"weight\": 0.75},\n",
    "    {\"city\": \"Chicago\", \"lat\": 41.8781, \"lon\": -87.6298, \"weight\": 0.55},\n",
    "    {\"city\": \"Houston\", \"lat\": 29.7604, \"lon\": -95.3698, \"weight\": 0.45},\n",
    "    {\"city\": \"Seattle\", \"lat\": 47.6062, \"lon\": -122.3321, \"weight\": 0.35},\n",
    "    {\"city\": \"Miami\", \"lat\": 25.7617, \"lon\": -80.1918, \"weight\": 0.30}\n",
    "]\n",
    "\n",
    "def km_to_deg(km: float) -> float:\n",
    "    # Approx conversion at mid-latitudes (rough but fine for synthetic data)\n",
    "    return km / 111.0\n",
    "\n",
    "def generate_raw_locations(n_users: int = 5000) -> pd.DataFrame:\n",
    "    weights = np.array([m[\"weight\"] for m in metros], dtype=float)\n",
    "    weights /= weights.sum()\n",
    "    city_choices = rng.choice(len(metros), size=n_users, p=weights)\n",
    "    records = []\n",
    "    for idx in city_choices:\n",
    "        m = metros[idx]\n",
    "        # Local jitter: normal noise around city center (roughly ~3km std dev)\n",
    "        lat = rng.normal(m[\"lat\"], km_to_deg(3.0))\n",
    "        lon = rng.normal(m[\"lon\"], km_to_deg(3.0))\n",
    "        records.append({\"city\": m[\"city\"], \"lat\": lat, \"lon\": lon})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "raw_df = generate_raw_locations()\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540d363",
   "metadata": {},
   "source": [
    "## Step 2 — Differential privacy mechanisms on gridded counts\n",
    "We grid the bounding box, add Laplace noise to each cell count (sensitivity=1), and synthesize new points from noisy counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box around the metros for gridding\n",
    "lat_min, lat_max = raw_df.lat.min() - km_to_deg(10), raw_df.lat.max() + km_to_deg(10)\n",
    "lon_min, lon_max = raw_df.lon.min() - km_to_deg(10), raw_df.lon.max() + km_to_deg(10)\n",
    "\n",
    "def laplace_noise(scale: float, size) -> np.ndarray:\n",
    "    return rng.laplace(0.0, scale, size=size)\n",
    "\n",
    "def to_grid_indices(lat, lon, grid_size):\n",
    "    lat_idx = np.clip(((lat - lat_min) / (lat_max - lat_min) * grid_size).astype(int), 0, grid_size - 1)\n",
    "    lon_idx = np.clip(((lon - lon_min) / (lon_max - lon_min) * grid_size).astype(int), 0, grid_size - 1)\n",
    "    return lat_idx, lon_idx\n",
    "\n",
    "def grid_counts(df: pd.DataFrame, grid_size: int) -> np.ndarray:\n",
    "    counts = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    lat_idx, lon_idx = to_grid_indices(df.lat.values, df.lon.values, grid_size)\n",
    "    for i, j in zip(lat_idx, lon_idx):\n",
    "        counts[i, j] += 1\n",
    "    return counts\n",
    "\n",
    "def dp_grid_counts(df: pd.DataFrame, grid_size: int, epsilon: float) -> np.ndarray:\n",
    "    counts = grid_counts(df, grid_size)\n",
    "    scale = 1.0 / epsilon\n",
    "    noisy = counts + laplace_noise(scale=scale, size=counts.shape)\n",
    "    noisy = np.maximum(noisy, 0.0)\n",
    "    return noisy\n",
    "\n",
    "def sample_from_grid(noisy_counts: np.ndarray, n: int) -> tuple:\n",
    "    flat = noisy_counts.flatten().astype(float)\n",
    "    probs = flat / flat.sum()\n",
    "    chosen = rng.choice(len(flat), size=n, p=probs)\n",
    "    lat_idx = chosen // noisy_counts.shape[1]\n",
    "    lon_idx = chosen % noisy_counts.shape[1]\n",
    "    return lat_idx, lon_idx\n",
    "\n",
    "def cell_center(lat_idx, lon_idx, grid_size):\n",
    "    lat = lat_min + (lat_idx + 0.5) * (lat_max - lat_min) / grid_size\n",
    "    lon = lon_min + (lon_idx + 0.5) * (lon_max - lon_min) / grid_size\n",
    "    return lat, lon\n",
    "\n",
    "def jitter_point(lat_center, lon_center, std_km):\n",
    "    std_deg = km_to_deg(std_km)\n",
    "    return rng.normal(lat_center, std_deg), rng.normal(lon_center, std_deg)\n",
    "\n",
    "def generate_dp_synthetic(df: pd.DataFrame, cfg: DPConfig) -> pd.DataFrame:\n",
    "    noisy_counts = dp_grid_counts(df, cfg.grid_size, cfg.epsilon_counts)\n",
    "    lat_idx, lon_idx = sample_from_grid(noisy_counts, len(df))\n",
    "    syn_records = []\n",
    "    for i, j in zip(lat_idx, lon_idx):\n",
    "        lat_c, lon_c = cell_center(i, j, cfg.grid_size)\n",
    "        lat, lon = jitter_point(lat_c, lon_c, cfg.jitter_std_km)\n",
    "        syn_records.append({\"lat\": lat, \"lon\": lon})\n",
    "    return pd.DataFrame(syn_records)\n",
    "\n",
    "dp_counts = dp_grid_counts(raw_df, config.grid_size, config.epsilon_counts)\n",
    "syn_df = generate_dp_synthetic(raw_df, config)\n",
    "syn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ce4d0",
   "metadata": {},
   "source": [
    "## Step 3 — k-anonymity / l-diversity / t-closeness style checks\n",
    "We coarse-grain locations to the nearest metro to assess group sizes and distribution similarity. These checks are informational safeguards on top of DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_city(lat, lon):\n",
    "    dists = []\n",
    "    for m in metros:\n",
    "        d = (lat - m[\"lat\"]) ** 2 + (lon - m[\"lon\"]) ** 2\n",
    "        dists.append(d)\n",
    "    return metros[int(np.argmin(dists))][\"city\"]\n",
    "\n",
    "raw_df[\"bucket\"] = raw_df.apply(lambda r: nearest_city(r.lat, r.lon), axis=1)\n",
    "syn_df[\"bucket\"] = syn_df.apply(lambda r: nearest_city(r.lat, r.lon), axis=1)\n",
    "\n",
    "def k_anonymity(df: pd.DataFrame, k: int) -> bool:\n",
    "    sizes = df.groupby(\"bucket\").size()\n",
    "    return bool((sizes >= k).all())\n",
    "\n",
    "def l_diversity(df: pd.DataFrame, l: int) -> bool:\n",
    "    # City buckets are the quasi-identifier; diversity over grid-cell ids for illustration\n",
    "    lat_idx, lon_idx = to_grid_indices(df.lat.values, df.lon.values, config.grid_size)\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp[\"cell\"] = list(zip(lat_idx, lon_idx))\n",
    "    diverse = df_tmp.groupby(\"bucket\")[\"cell\"].nunique()\n",
    "    return bool((diverse >= l).all())\n",
    "\n",
    "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    def kl(a, b):\n",
    "        mask = (a > 0) & (b > 0)\n",
    "        return np.sum(a[mask] * np.log(a[mask] / b[mask]))\n",
    "    return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
    "\n",
    "def t_closeness(raw: pd.Series, syn: pd.Series, threshold: float) -> bool:\n",
    "    common = sorted(set(raw.index).union(set(syn.index)))\n",
    "    p = raw.reindex(common, fill_value=0).to_numpy(dtype=float)\n",
    "    q = syn.reindex(common, fill_value=0).to_numpy(dtype=float)\n",
    "    js = js_divergence(p, q)\n",
    "    return bool(js <= threshold), js\n",
    "\n",
    "raw_bucket_counts = raw_df.groupby(\"bucket\").size()\n",
    "syn_bucket_counts = syn_df.groupby(\"bucket\").size()\n",
    "\n",
    "k_ok = k_anonymity(syn_df, config.k_anonymity_k)\n",
    "l_ok = l_diversity(syn_df, config.l_diversity_l)\n",
    "t_ok, t_js = t_closeness(raw_bucket_counts, syn_bucket_counts, config.t_closeness_thresh)\n",
    "\n",
    "privacy_checks = {\"k_anonymity\": k_ok, \"l_diversity\": l_ok, \"t_closeness_ok\": t_ok, \"t_closeness_js\": t_js}\n",
    "privacy_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0d5a3",
   "metadata": {},
   "source": [
    "## Step 4 — Utility evaluation\n",
    "We compare bucket distributions (JS divergence), spatial error (km), and show a small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7327f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # Great-circle distance in kilometers\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    return 6371.0 * 2 * asin(sqrt(a))\n",
    "\n",
    "def spatial_error(raw: pd.DataFrame, syn: pd.DataFrame, sample: int = 2000) -> float:\n",
    "    m = min(sample, len(raw), len(syn))\n",
    "    raw_sample = raw.sample(m, random_state=config.seed)\n",
    "    syn_sample = syn.sample(m, random_state=config.seed)\n",
    "    errs = [haversine_km(r.lat, r.lon, s.lat, s.lon) for (_, r), (_, s) in zip(raw_sample.iterrows(), syn_sample.iterrows())]\n",
    "    return float(np.mean(errs))\n",
    "\n",
    "js_global = js_divergence(raw_bucket_counts.to_numpy(dtype=float), syn_bucket_counts.to_numpy(dtype=float))\n",
    "avg_km_error = spatial_error(raw_df, syn_df)\n",
    "\n",
    "utility_report = pd.DataFrame({\n",
    "    \"metric\": [\"JS divergence (buckets)\", \"Avg spatial error (km)\"],\n",
    "    \"value\": [js_global, avg_km_error]\n",
    "})\n",
    "utility_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf5f67",
   "metadata": {},
   "source": [
    "## Step 5 — Visual comparisons (bottom cells per README)\n",
    "Plots help eyeball that synthetic data follows high-level geography without revealing individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "sns.kdeplot(data=raw_df, x=\"lon\", y=\"lat\", fill=True, cmap=\"Blues\", levels=25, ax=axes[0])\n",
    "axes[0].set_title(\"Raw (non-private synthetic)\")\n",
    "sns.kdeplot(data=syn_df, x=\"lon\", y=\"lat\", fill=True, cmap=\"Greens\", levels=25, ax=axes[1])\n",
    "axes[1].set_title(\"DP synthetic (released)\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9176a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side bucket counts\n",
    "bucket_compare = pd.DataFrame({\n",
    "    \"bucket\": raw_bucket_counts.index,\n",
    "    \"raw_count\": raw_bucket_counts.values,\n",
    "    \"dp_syn_count\": syn_bucket_counts.reindex(raw_bucket_counts.index).values\n",
    "}).set_index(\"bucket\")\n",
    "bucket_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24909708",
   "metadata": {},
   "source": [
    "## How to use / extend\n",
    "- Adjust `epsilon_counts` and `epsilon_release` to trade privacy vs. fidelity (smaller epsilon = more noise).\n",
    "- Change `grid_size` to tune spatial granularity; larger grids may need larger epsilon to stay useful.\n",
    "- Swap the metro list for any bounding box you need; raw data generator stays synthetic.\n",
    "- Add categorical attributes (e.g., venue type) and apply the same DP histogram + sampling pattern per attribute combination.\n",
    "- Persist outputs: `syn_df.to_csv('dp_synthetic_locations.csv', index=False)` if you need a release artifact."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
